The activation parameter influences, as best i can tell the activation function that is responsible for non linearity. Changing it should should therfore influence how well the model learns, its ability to interact with non linarity. The parameter will be used for the fully connected parts, which have a large influence on how the model functions.
Possible values are: relu, sigmoid, tanh, linear, LeakyReLU, softmax.

The value linear will propably mean the model will not handle non linear information well.
As for the others i can not make a guess as to their effect.

My aproach will be to copy the code all the way to the evaluation and create variations with 5 different values. Since i have found 6 Values i will use all exept LeakyReLU, since i can not remember the fix, how to get that to work, at the moment.


relu: 
	gradual emprovement that relativly 		smoothly tapers of.
	Epochs 16
	Best Epoch: val_accuracy: 0.9375 - 		val_loss: 0.2281 
	12 falls predictions

sigmoid:
	Sharp increas in imprvement than very 		shallow improvement. 
	24 Epochs
	Best Epoch: Epoch 23: val_accuracy: 		0.9453 - val_loss: 0.1771	
	8 falls predictions
tanh:
	Smooth and oftapering imrpovement
	18 Epochs
	Best Epoch: Epoch 18 val_accuracy: 		0.9453 - val_loss: 0.1705
	7 falls predictions
linear:
	Big jumps in accuracy
	24 Epochs
	Best Epoch: Epoch 24: val_accuracy: 		0.9453 - val_loss: 0.2313 
	7 falls predictions
softmax:
	Dose not get better.
	10 Epochs
	Best Epoch: Epoch 4: 0.4219 -val_loss: 		1.0741
	83 falls predictions


